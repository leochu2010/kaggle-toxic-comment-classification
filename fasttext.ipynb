{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from pyfasttext import FastText\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('input/train.csv')\n",
    "test = pd.read_csv('input/test.csv')\n",
    "\n",
    "COMMENT = 'comment_text'\n",
    "train[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "test[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "train[COMMENT] = train[COMMENT].str.lower().str.replace('https?:\\/\\/[^\\s]*','').str.replace(\"’\",\"'\").str.replace(\"''\",' ').str.replace(\"'\",\" ' \").str.replace('“','\"').str.replace('”','\"').str.replace('\"',' : ').str.replace('.',' . ').str.replace(',',' , ').str.replace('[',' [ ').str.replace(']',' ] ').str.replace('(',' ( ').str.replace(')',' ) ').str.replace('!',' ! ').str.replace('?',' ? ').str.replace(';',' ').str.replace(':',' ').str.replace('-',' - ').str.replace('=', ' ').str.replace('=', ' ').str.replace('*', ' ').str.replace('|', ' ').str.replace('«', ' ').str.replace('\\d', ' ').str.replace('\\n', ' ').str.replace('\\s\\s+',' ').str.replace('\\n','NEWLINE ').str.strip().str.replace(\"[^\\s\\d\\w)(;:\\-+_?!\\]\\[,*,'\\\"]+\",\"\")\n",
    "test[COMMENT] = test[COMMENT].str.lower().str.replace('https?:\\/\\/[^\\s]*','').str.replace(\"’\",\"'\").str.replace(\"''\",' ').str.replace(\"'\",\" ' \").str.replace('“','\"').str.replace('”','\"').str.replace('\"',' : ').str.replace('.',' . ').str.replace(',',' , ').str.replace('[',' [ ').str.replace(']',' ] ').str.replace('(',' ( ').str.replace(')',' ) ').str.replace('!',' ! ').str.replace('?',' ? ').str.replace(';',' ').str.replace(':',' ').str.replace('-',' - ').str.replace('=', ' ').str.replace('=', ' ').str.replace('*', ' ').str.replace('|', ' ').str.replace('«', ' ').str.replace('\\d', ' ').str.replace('\\n', ' ').str.replace('\\s\\s+',' ').str.replace('\\n','NEWLINE ').str.strip().str.replace(\"[^\\s\\d\\w)(;:\\-+_?!\\]\\[,*,'\\\"]+\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9.585100e+04</td>\n",
       "      <td>95851.000000</td>\n",
       "      <td>95851.000000</td>\n",
       "      <td>95851.000000</td>\n",
       "      <td>95851.000000</td>\n",
       "      <td>95851.000000</td>\n",
       "      <td>95851.000000</td>\n",
       "      <td>95851.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.994359e+11</td>\n",
       "      <td>0.096368</td>\n",
       "      <td>0.010068</td>\n",
       "      <td>0.053301</td>\n",
       "      <td>0.003182</td>\n",
       "      <td>0.049713</td>\n",
       "      <td>0.008492</td>\n",
       "      <td>0.897862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.890136e+11</td>\n",
       "      <td>0.295097</td>\n",
       "      <td>0.099832</td>\n",
       "      <td>0.224635</td>\n",
       "      <td>0.056320</td>\n",
       "      <td>0.217352</td>\n",
       "      <td>0.091762</td>\n",
       "      <td>0.302831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.225664e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.473437e+11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.001297e+11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.501088e+11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.999882e+11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id         toxic  severe_toxic       obscene        threat  \\\n",
       "count  9.585100e+04  95851.000000  95851.000000  95851.000000  95851.000000   \n",
       "mean   4.994359e+11      0.096368      0.010068      0.053301      0.003182   \n",
       "std    2.890136e+11      0.295097      0.099832      0.224635      0.056320   \n",
       "min    2.225664e+07      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    2.473437e+11      0.000000      0.000000      0.000000      0.000000   \n",
       "50%    5.001297e+11      0.000000      0.000000      0.000000      0.000000   \n",
       "75%    7.501088e+11      0.000000      0.000000      0.000000      0.000000   \n",
       "max    9.999882e+11      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "             insult  identity_hate         clean  \n",
       "count  95851.000000   95851.000000  95851.000000  \n",
       "mean       0.049713       0.008492      0.897862  \n",
       "std        0.217352       0.091762      0.302831  \n",
       "min        0.000000       0.000000      0.000000  \n",
       "25%        0.000000       0.000000      1.000000  \n",
       "50%        0.000000       0.000000      1.000000  \n",
       "75%        0.000000       0.000000      1.000000  \n",
       "max        1.000000       1.000000      1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train['clean'] = 1-train[label_cols].max(axis=1)\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pretrain vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## youtube comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_youtube_comments_df():\n",
    "    youtube_gb_comments = pd.read_csv('GBcomments.csv', error_bad_lines = False)\n",
    "    youtube_us_comments = pd.read_csv('UScomments.csv', error_bad_lines = False)\n",
    "\n",
    "    youtube_gb_comments[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "    youtube_gb_comments[COMMENT] = youtube_gb_comments[COMMENT].str.lower().str.replace('https?:\\/\\/[^\\s]*','').str.replace(\"’\",\"'\").str.replace(\"''\",' ').str.replace(\"'\",\" ' \").str.replace('“','\"').str.replace('”','\"').str.replace('\"',' : ').str.replace('.',' . ').str.replace(',',' , ').str.replace('[',' [ ').str.replace(']',' ] ').str.replace('(',' ( ').str.replace(')',' ) ').str.replace('!',' ! ').str.replace('?',' ? ').str.replace(';',' ').str.replace(':',' ').str.replace('-',' - ').str.replace('=', ' ').str.replace('=', ' ').str.replace('*', ' ').str.replace('|', ' ').str.replace('«', ' ').str.replace('\\d', ' ').str.replace('\\n', ' ').str.replace('\\s\\s+',' ').str.replace('\\n','NEWLINE ').str.strip().str.replace(\"[^\\s\\d\\w)(;:\\-+_?!\\]\\[,*,'\\\"]+\",\"\")\n",
    "    youtube_us_comments[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "    youtube_us_comments[COMMENT] = youtube_us_comments[COMMENT].str.lower().str.replace('https?:\\/\\/[^\\s]*','').str.replace(\"’\",\"'\").str.replace(\"''\",' ').str.replace(\"'\",\" ' \").str.replace('“','\"').str.replace('”','\"').str.replace('\"',' : ').str.replace('.',' . ').str.replace(',',' , ').str.replace('[',' [ ').str.replace(']',' ] ').str.replace('(',' ( ').str.replace(')',' ) ').str.replace('!',' ! ').str.replace('?',' ? ').str.replace(';',' ').str.replace(':',' ').str.replace('-',' - ').str.replace('=', ' ').str.replace('=', ' ').str.replace('*', ' ').str.replace('|', ' ').str.replace('«', ' ').str.replace('\\d', ' ').str.replace('\\n', ' ').str.replace('\\s\\s+',' ').str.replace('\\n','NEWLINE ').str.strip().str.replace(\"[^\\s\\d\\w)(;:\\-+_?!\\]\\[,*,'\\\"]+\",\"\")\n",
    "\n",
    "    youtube_gb_comments.drop(['video_id','likes','replies'],1, inplace=True)\n",
    "    youtube_us_comments.drop(['video_id','likes','replies'],1, inplace=True)\n",
    "    return pd.concat([youtube_gb_comments,youtube_us_comments])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_toxicity_df():\n",
    "    toxicity_annotated_comments = pd.read_csv('toxicity_annotated_comments.tsv', sep='\\t')\n",
    "    toxicity_annotations = pd.read_csv('toxicity_annotations.tsv', sep='\\t')\n",
    "    rev_toxicity = toxicity_annotations.drop(['worker_id'],1).groupby(['rev_id']).mean().reset_index()\n",
    "    toxicity_df = toxicity_annotated_comments.merge(rev_toxicity, left_on='rev_id', right_on='rev_id', how='inner')[['comment','toxicity']]\n",
    "    # add toxicity to train\n",
    "    # reshape toxicity into train\n",
    "    toxicity_df = toxicity_df.rename(index=str, columns={'comment': 'comment_text'})\n",
    "    toxicity_df['toxic'] = toxicity_df['toxicity'] > 0.5\n",
    "    toxicity_df['toxic'] = toxicity_df['toxic'].apply(lambda x: 1 if x else 0)\n",
    "    toxicity_df['id'] = 0\n",
    "    toxicity_df['severe_toxic'] = 0\n",
    "    toxicity_df['obscene'] = 0\n",
    "    toxicity_df['threat'] = 0\n",
    "    toxicity_df['insult'] = 0\n",
    "    toxicity_df['identity_hate'] = 0\n",
    "    toxicity_df[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "    toxicity_df[COMMENT] = toxicity_df[COMMENT].str.lower().str.replace('https?:\\/\\/[^\\s]*','').str.replace(\"’\",\"'\").str.replace(\"''\",' ').str.replace(\"'\",\" ' \").str.replace('“','\"').str.replace('”','\"').str.replace('\"',' : ').str.replace('.',' . ').str.replace(',',' , ').str.replace('[',' [ ').str.replace(']',' ] ').str.replace('(',' ( ').str.replace(')',' ) ').str.replace('!',' ! ').str.replace('?',' ? ').str.replace(';',' ').str.replace(':',' ').str.replace('-',' - ').str.replace('=', ' ').str.replace('=', ' ').str.replace('*', ' ').str.replace('|', ' ').str.replace('«', ' ').str.replace('\\d', ' ').str.replace('\\n', ' ').str.replace('\\s\\s+',' ').str.replace('\\n','NEWLINE ').str.strip().str.replace(\"[^\\s\\d\\w)(;:\\-+_?!\\]\\[,*,'\\\"]+\",\"\")\n",
    "    toxicity_df.drop(['toxicity'],1,inplace=True)\n",
    "    toxicity_df['clean'] = 1-toxicity_df[label_cols].max(axis=1)\n",
    "    return toxicity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO wiki comments corpus https://figshare.com/articles/Wikipedia_Talk_Corpus/4264973"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 113225: expected 4 fields, saw 5\\n'\n",
      "b'Skipping line 158379: expected 4 fields, saw 7\\nSkipping line 241590: expected 4 fields, saw 5\\nSkipping line 245637: expected 4 fields, saw 7\\n'\n",
      "b'Skipping line 521402: expected 4 fields, saw 5\\n'\n",
      "b'Skipping line 41589: expected 4 fields, saw 11\\nSkipping line 51628: expected 4 fields, saw 7\\nSkipping line 114465: expected 4 fields, saw 5\\n'\n",
      "b'Skipping line 142496: expected 4 fields, saw 8\\nSkipping line 189732: expected 4 fields, saw 6\\nSkipping line 245218: expected 4 fields, saw 7\\n'\n",
      "b'Skipping line 388430: expected 4 fields, saw 5\\n'\n",
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:2850: DtypeWarning: Columns (2,3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_toxic_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-105805004d93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0myoutube_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_youtube_comments_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtoxic_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_toxic_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'get_toxic_df' is not defined"
     ]
    }
   ],
   "source": [
    "youtube_df = get_youtube_comments_df()\n",
    "toxic_df = get_toxicity_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_comment = train.copy()\n",
    "#train_comment.drop(label_cols, 1, inplace=True)\n",
    "#train_comment.drop(['clean'],1, inplace=True)\n",
    "#train_comment.to_csv(\"train_comments.csv\", columns=['comment_text'], index = False)\n",
    "all_comment = pd.concat([train,test,youtube_df,toxic_df])\n",
    "all_comment.to_csv('all_comments.csv', columns=['comment_text'], index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_wiki_youtube_comment = pd.concat([train_comment,test,youtube_df])\n",
    "#all_wiki_youtube_comment.to_csv('all_wiki_youtube_comments.csv', columns=['comment_text'], index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText()\n",
    "model.skipgram(input='all_comments.csv', output='comments_min2_max5_word2_epoch_10', minn='2', maxn='5', wordNgrams='2', epoch='1', dim='150')\n",
    "#model.skipgram(input='all_wiki_youtube_comments.csv', output='awy_comments_skipgram', minn='1', maxn='5', wordNgrams='3', epoch='1')\n",
    "#model = FastText()\n",
    "del model\n",
    "#model.skipgram(input='train_comments.csv', output='train_comments_skipgram_1_1_2', minn='1', maxn='1', wordNgrams='2')\n",
    "#del model\n",
    "#model1 = FastText()\n",
    "#model1.cbow(input='train_comments.csv', output='train_comments_cbow', minn='1', maxn='5', wordNgrams='2')\n",
    "#del model1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/pandas/core/indexing.py:537: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "train_df = None\n",
    "val_df = None\n",
    "#toxicity_df = get_toxicity_df() \n",
    "for label in label_cols:\n",
    "    train['label'] = \"\"    \n",
    "    train_df, val_df = train_test_split(train, test_size=0.2, random_state=1)\n",
    "    # merge toxicity before training\n",
    "    #train_df = pd.concat([train_df, toxicity_df])\n",
    "    train_df.loc[(train_df[label]==1), 'label'] = \"__label__\" + label + \" \"\n",
    "    train_df.loc[(train_df[label]==0), 'label'] = \"__label__clean\" + \" \"\n",
    "    val_df.loc[(val_df[label]==1), 'label'] = \"__label__\" + label + \" \"\n",
    "    val_df.loc[(val_df[label]==0), 'label'] = \"__label__clean\" + \" \"\n",
    "    train_df.to_csv(\"fasttext_train_\"+label+\".csv\", columns=['label', 'comment_text'], index=False)    \n",
    "    train.to_csv(\"fasttext_train_all_\"+label+\".csv\", columns=['label', 'comment_text'], index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_name(target, param):\n",
    "    name = target\n",
    "    for key in param.keys():\n",
    "        name += \"_\"+key+str(param[key])        \n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(param, target, all_sample = False):\n",
    "    model = FastText()    \n",
    "    if all_sample == False:\n",
    "        model.supervised(input='fasttext_train_'+target+'.csv', output='parameter_search', epoch=param[\"epoch\"], minn=param[\"minn\"], maxn=param[\"maxn\"], wordNgrams=param[\"wordNgrams\"], dim=param[\"dim\"], neg=param[\"neg\"], pretrainedVectors=param[\"pretrainedVectors\"])\n",
    "        #model.supervised(input='fasttext_train_'+target+'.csv', output='parameter_search', epoch=param[\"epoch\"], minn=param[\"minn\"], maxn=param[\"maxn\"], wordNgrams=param[\"wordNgrams\"], dim=param[\"dim\"], neg=param[\"neg\"])\n",
    "    else:\n",
    "        model.supervised(input='fasttext_train_all_'+target+'.csv', output='parameter_search', epoch=param[\"epoch\"], minn=param[\"minn\"], maxn=param[\"maxn\"], wordNgrams=param[\"wordNgrams\"], dim=param[\"dim\"], neg=param[\"neg\"], pretrainedVectors=param[\"pretrainedVectors\"])\n",
    "        #model.supervised(input='fasttext_train_all_'+target+'.csv', output='parameter_search', epoch=param[\"epoch\"], minn=param[\"minn\"], maxn=param[\"maxn\"], wordNgrams=param[\"wordNgrams\"], dim=param[\"dim\"], neg=param[\"neg\"])        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_log_loss(model, target):\n",
    "    predict_probas = model.predict_proba(val_df[COMMENT],k=2) \n",
    "    pred = []\n",
    "    for predict in predict_probas:\n",
    "        pred_prob = 0\n",
    "        for label, prob in predict:\n",
    "            if label == target:         \n",
    "                pred_prob = prob\n",
    "        pred.append(pred_prob) \n",
    "    return log_loss(val_df[target],pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter search toxic\n",
      "parameter search severe_toxic\n",
      "parameter search obscene\n",
      "parameter search threat\n",
      "parameter search insult\n",
      "parameter search identity_hate\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "param_grid = {}\n",
    "param_grid[\"minn\"] = [1,2]\n",
    "param_grid[\"maxn\"] = [2,3,4,5]\n",
    "param_grid[\"epoch\"] = [5,6,7,8,10,15]\n",
    "param_grid[\"lr\"] = [0.1,0.5,1]\n",
    "param_grid[\"wordNgrams\"] =[1,2,3,4,5]\n",
    "param_grid[\"dim\"] =[100,150,200]\n",
    "targets = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "log_loss_df_columns = ['target','log_loss','model_name']\n",
    "for key in param_grid.keys():\n",
    "    log_loss_df_columns.append(key)\n",
    "\n",
    "log_loss_df = None\n",
    "try:\n",
    "    log_loss_df = pd.read_csv('parameter_random_search.csv')    \n",
    "except:\n",
    "    print(\"parameter_random_search.csv not found\")\n",
    "    log_loss_df = pd.DataFrame(columns=log_loss_df_columns)\n",
    "\n",
    "tested_params = log_loss_df['model_name'].tolist()\n",
    "\n",
    "for target in targets:\n",
    "    print(\"parameter search\",target)    \n",
    "    max_test = len(tested_params) + 10\n",
    "    while len(tested_params) < max_test:        \n",
    "        param = random.choice(list(ParameterGrid(param_grid)))                     \n",
    "        test_model_name = model_name(target, param)\n",
    "        if test_model_name in tested_params:\n",
    "            continue\n",
    "        else:\n",
    "            tested_params.append(test_model_name)\n",
    "            \n",
    "        model = train_model(param, target)\n",
    "        score = calculate_log_loss(model, target)\n",
    "        \n",
    "        log_loss_df_row = {}\n",
    "        log_loss_df_row['target'] = target\n",
    "        log_loss_df_row['log_loss'] = score\n",
    "        log_loss_df_row['model_name'] = test_model_name\n",
    "        for key in param.keys():\n",
    "            log_loss_df_row[key] = param[key]\n",
    "        log_loss_df = log_loss_df.append(log_loss_df_row, ignore_index=True)        \n",
    "        log_loss_df.to_csv(\"parameter_random_search.csv\", index=False)    \n",
    "        print(score, param)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter search toxic\n",
      "0.187266459037 {'maxn': 5, 'lr': 0.0001, 'epoch': 1, 'minn': 2, 'dim': 200, 'wordNgrams': 2, 'pretrainedVectors': 'wiki_comments/inputtest_article_comments_nosurf_dim200.vec', 'neg': 0}\n",
      "parameter search severe_toxic\n",
      "0.0405078810351 {'maxn': 5, 'lr': 0.0001, 'epoch': 1, 'minn': 2, 'dim': 200, 'wordNgrams': 2, 'pretrainedVectors': 'wiki_comments/inputtest_article_comments_nosurf_dim200.vec', 'neg': 0}\n",
      "parameter search obscene\n",
      "0.108811285734 {'maxn': 5, 'lr': 0.0001, 'epoch': 1, 'minn': 2, 'dim': 200, 'wordNgrams': 2, 'pretrainedVectors': 'wiki_comments/inputtest_article_comments_nosurf_dim200.vec', 'neg': 0}\n",
      "parameter search threat\n",
      "0.0172615014757 {'maxn': 5, 'lr': 0.0001, 'epoch': 1, 'minn': 2, 'dim': 200, 'wordNgrams': 2, 'pretrainedVectors': 'wiki_comments/inputtest_article_comments_nosurf_dim200.vec', 'neg': 0}\n",
      "parameter search insult\n",
      "0.128582444098 {'maxn': 5, 'lr': 0.0001, 'epoch': 1, 'minn': 2, 'dim': 200, 'wordNgrams': 2, 'pretrainedVectors': 'wiki_comments/inputtest_article_comments_nosurf_dim200.vec', 'neg': 0}\n",
      "parameter search identity_hate\n",
      "0.0418522815142 {'maxn': 5, 'lr': 0.0001, 'epoch': 1, 'minn': 2, 'dim': 200, 'wordNgrams': 2, 'pretrainedVectors': 'wiki_comments/inputtest_article_comments_nosurf_dim200.vec', 'neg': 0}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {}\n",
    "param_grid[\"minn\"] = [2]\n",
    "param_grid[\"maxn\"] = [5]\n",
    "param_grid[\"epoch\"] = [1]\n",
    "param_grid[\"lr\"] = [0.0001]\n",
    "param_grid[\"wordNgrams\"] =[2]\n",
    "param_grid[\"dim\"] = [200]\n",
    "param_grid[\"neg\"] = [0]\n",
    "param_grid[\"pretrainedVectors\"]=['wiki_comments/inputtest_article_comments_nosurf_dim200.vec']\n",
    "targets = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "#targets = ['toxic']\n",
    "#comments_min2_max5_word2_epoch_10\n",
    "\n",
    "log_loss_df_columns = ['target','log_loss']\n",
    "for key in param_grid.keys():\n",
    "    log_loss_df_columns.append(key)\n",
    "\n",
    "log_loss_df = pd.DataFrame(columns=log_loss_df_columns)\n",
    "\n",
    "for target in targets:\n",
    "    print(\"parameter search\",target)    \n",
    "    for param in list(ParameterGrid(param_grid)):                     \n",
    "        model = train_model(param, target)\n",
    "        score = calculate_log_loss(model, target)\n",
    "        \n",
    "        log_loss_df_row = {}\n",
    "        log_loss_df_row['target'] = target\n",
    "        log_loss_df_row['log_loss'] = score\n",
    "        for key in param.keys():\n",
    "            log_loss_df_row[key] = param[key]\n",
    "        log_loss_df = log_loss_df.append(log_loss_df_row, ignore_index=True)        \n",
    "        log_loss_df.to_csv(\"parameter_grid_search.csv\", index=False)    \n",
    "        print(score, param)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.087380308815817334"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss_df.groupby('target')['log_loss'].min().sum()/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "identity_hate    0.041852\n",
       "insult           0.128582\n",
       "obscene          0.108811\n",
       "severe_toxic     0.040508\n",
       "threat           0.017262\n",
       "toxic            0.187266\n",
       "Name: log_loss, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss_df.groupby('target')['log_loss'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>log_loss</th>\n",
       "      <th>minn</th>\n",
       "      <th>lr</th>\n",
       "      <th>epoch</th>\n",
       "      <th>maxn</th>\n",
       "      <th>wordNgrams</th>\n",
       "      <th>dim</th>\n",
       "      <th>pretrainedVectors</th>\n",
       "      <th>neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.041852</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>wiki_comments/inputtest_article_comments_nosur...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          target  log_loss minn      lr epoch maxn wordNgrams  dim  \\\n",
       "5  identity_hate  0.041852    2  0.0001     1    5          2  200   \n",
       "\n",
       "                                   pretrainedVectors neg  \n",
       "5  wiki_comments/inputtest_article_comments_nosur...   0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "log_loss_df[log_loss_df['target']=='identity_hate'].sort_values('log_loss',ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {}\n",
    "target='toxic'\n",
    "param['toxic'] = {'maxn': 1, 'wordNgrams': 2, 'lr': 0.5, 'minn': 1, 'epoch': 5, 'dim': 100}\n",
    "model = train_model(param[target], target, all_sample = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['clean', 'toxic']\n",
      "['clean', 'severe_toxic']\n",
      "['clean', 'obscene']\n",
      "['clean', 'threat']\n",
      "['clean', 'insult']\n",
      "['clean', 'identity_hate']\n"
     ]
    }
   ],
   "source": [
    "comment_ids = []\n",
    "comments = []\n",
    "for index, row in test.iterrows():\n",
    "    comments.append(row['comment_text'])\n",
    "    comment_ids.append(row['id'])\n",
    "\n",
    "predict_probas = {}\n",
    "param = {}\n",
    "param['toxic'] = {'maxn': 5, 'wordNgrams': 3, 'lr': 0.5, 'minn': 1, 'epoch': 8, 'dim': 100}\n",
    "param['severe_toxic'] = {'maxn': 5, 'wordNgrams': 4, 'lr': 0.5, 'minn': 2, 'epoch': 5, 'dim': 100}\n",
    "param['obscene'] = {'maxn': 5, 'wordNgrams': 1, 'lr': 0.1, 'minn': 1, 'epoch': 8, 'dim': 100}\n",
    "param['threat'] = {'maxn': 4, 'wordNgrams': 5, 'lr': 1, 'minn': 1, 'epoch': 10, 'dim': 100}\n",
    "param['insult'] = {'maxn': 5, 'wordNgrams': 3, 'lr': 0.5, 'minn': 1, 'epoch': 5, 'dim': 100}\n",
    "param['identity_hate'] = {'maxn': 5, 'wordNgrams': 3, 'lr': 1, 'minn': 2, 'epoch': 6, 'dim': 100}\n",
    "\n",
    "for target in label_cols:\n",
    "    model = train_model(param[target], target, all_sample = True)\n",
    "    print(model.labels)\n",
    "    predict_probas[target] = model.predict_proba(comments,k=2)   \n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('submit.csv', \"w\") as csv_file:\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    writer.writerow(['id','toxic','severe_toxic','obscene','threat','insult','identity_hate'])\n",
    "    csv_rows = []\n",
    "    for index, comment_id in enumerate(comment_ids):\n",
    "        #print('processing',index)\n",
    "        prob_dict = {}        \n",
    "        #print(comments[index])\n",
    "        for label in label_cols:       \n",
    "            #print(predict_probas[label][index])            \n",
    "            for label_predict, prob in predict_probas[label][index]:\n",
    "                if label_predict != 'clean':    \n",
    "                    prob_dict[label] = prob        \n",
    "        csv_row=[]\n",
    "        csv_row.append(comment_id)\n",
    "        \n",
    "        # severe_toxic -> toxic trick                \n",
    "        #if 'severe_toxic' in prob_dict and 'toxic' in prob_dict and prob_dict['severe_toxic'] > prob_dict['toxic']:            \n",
    "        #    prob_dict['toxic'] = prob_dict['severe_toxic']            \n",
    "        \n",
    "        for label in label_cols:\n",
    "            if label in prob_dict:\n",
    "                csv_row.append(prob_dict[label])\n",
    "            else:\n",
    "                csv_row.append(0)\n",
    "                #print('no prediction:',index,'comment:',comments[index])\n",
    "                #csv_row.append(prob_dict[label])\n",
    "        csv_rows.append(csv_row)    \n",
    "    writer.writerows(csv_rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
