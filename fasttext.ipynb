{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from pyfasttext import FastText\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('input/train.csv')\n",
    "test = pd.read_csv('input/test.csv')\n",
    "\n",
    "COMMENT = 'comment_text'\n",
    "train[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "test[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "train[COMMENT] = train[COMMENT].str.lower().str.replace('https?:\\/\\/[^\\s]*','').str.replace(\"’\",\"'\").str.replace(\"''\",' ').str.replace(\"'\",\" ' \").str.replace('“','\"').str.replace('”','\"').str.replace('\"',' : ').str.replace('.',' . ').str.replace(',',' , ').str.replace('[',' [ ').str.replace(']',' ] ').str.replace('(',' ( ').str.replace(')',' ) ').str.replace('!',' ! ').str.replace('?',' ? ').str.replace(';',' ').str.replace(':',' ').str.replace('-',' - ').str.replace('=', ' ').str.replace('=', ' ').str.replace('*', ' ').str.replace('|', ' ').str.replace('«', ' ').str.replace('\\d', ' ').str.replace('\\n', ' ').str.replace('\\s\\s+',' ').str.strip().str.replace(\"[^\\s\\d\\w)(;:\\-+_?!\\]\\[,*,'\\\"]+\",\"\")\n",
    "test[COMMENT] = test[COMMENT].str.lower().str.replace('https?:\\/\\/[^\\s]*','').str.replace(\"’\",\"'\").str.replace(\"''\",' ').str.replace(\"'\",\" ' \").str.replace('“','\"').str.replace('”','\"').str.replace('\"',' : ').str.replace('.',' . ').str.replace(',',' , ').str.replace('[',' [ ').str.replace(']',' ] ').str.replace('(',' ( ').str.replace(')',' ) ').str.replace('!',' ! ').str.replace('?',' ? ').str.replace(';',' ').str.replace(':',' ').str.replace('-',' - ').str.replace('=', ' ').str.replace('=', ' ').str.replace('*', ' ').str.replace('|', ' ').str.replace('«', ' ').str.replace('\\d', ' ').str.replace('\\n', ' ').str.replace('\\s\\s+',' ').str.strip().str.replace(\"[^\\s\\d\\w)(;:\\-+_?!\\]\\[,*,'\\\"]+\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9.585100e+04</td>\n",
       "      <td>95851.000000</td>\n",
       "      <td>95851.000000</td>\n",
       "      <td>95851.000000</td>\n",
       "      <td>95851.000000</td>\n",
       "      <td>95851.000000</td>\n",
       "      <td>95851.000000</td>\n",
       "      <td>95851.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.994359e+11</td>\n",
       "      <td>0.096368</td>\n",
       "      <td>0.010068</td>\n",
       "      <td>0.053301</td>\n",
       "      <td>0.003182</td>\n",
       "      <td>0.049713</td>\n",
       "      <td>0.008492</td>\n",
       "      <td>0.897862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.890136e+11</td>\n",
       "      <td>0.295097</td>\n",
       "      <td>0.099832</td>\n",
       "      <td>0.224635</td>\n",
       "      <td>0.056320</td>\n",
       "      <td>0.217352</td>\n",
       "      <td>0.091762</td>\n",
       "      <td>0.302831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.225664e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.473437e+11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.001297e+11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.501088e+11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.999882e+11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id         toxic  severe_toxic       obscene        threat  \\\n",
       "count  9.585100e+04  95851.000000  95851.000000  95851.000000  95851.000000   \n",
       "mean   4.994359e+11      0.096368      0.010068      0.053301      0.003182   \n",
       "std    2.890136e+11      0.295097      0.099832      0.224635      0.056320   \n",
       "min    2.225664e+07      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    2.473437e+11      0.000000      0.000000      0.000000      0.000000   \n",
       "50%    5.001297e+11      0.000000      0.000000      0.000000      0.000000   \n",
       "75%    7.501088e+11      0.000000      0.000000      0.000000      0.000000   \n",
       "max    9.999882e+11      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "             insult  identity_hate         clean  \n",
       "count  95851.000000   95851.000000  95851.000000  \n",
       "mean       0.049713       0.008492      0.897862  \n",
       "std        0.217352       0.091762      0.302831  \n",
       "min        0.000000       0.000000      0.000000  \n",
       "25%        0.000000       0.000000      1.000000  \n",
       "50%        0.000000       0.000000      1.000000  \n",
       "75%        0.000000       0.000000      1.000000  \n",
       "max        1.000000       1.000000      1.000000  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train['clean'] = 1-train[label_cols].max(axis=1)\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pretrain vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## youtube comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_youtube_comments_df():\n",
    "    youtube_gb_comments = pd.read_csv('GBcomments.csv', error_bad_lines = False)\n",
    "    youtube_us_comments = pd.read_csv('UScomments.csv', error_bad_lines = False)\n",
    "\n",
    "    youtube_gb_comments[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "    youtube_gb_comments[COMMENT] = youtube_gb_comments[COMMENT].str.lower().str.replace('https?:\\/\\/[^\\s]*','').str.replace(\"’\",\"'\").str.replace(\"''\",' ').str.replace(\"'\",\" ' \").str.replace('“','\"').str.replace('”','\"').str.replace('\"',' : ').str.replace('.',' . ').str.replace(',',' , ').str.replace('[',' [ ').str.replace(']',' ] ').str.replace('(',' ( ').str.replace(')',' ) ').str.replace('!',' ! ').str.replace('?',' ? ').str.replace(';',' ').str.replace(':',' ').str.replace('-',' - ').str.replace('=', ' ').str.replace('=', ' ').str.replace('*', ' ').str.replace('|', ' ').str.replace('«', ' ').str.replace('\\d', ' ').str.replace('\\n', ' ').str.replace('\\s\\s+',' ').str.strip().str.replace(\"[^\\s\\d\\w)(;:\\-+_?!\\]\\[,*,'\\\"]+\",\"\")\n",
    "    youtube_us_comments[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "    youtube_us_comments[COMMENT] = youtube_us_comments[COMMENT].str.lower().str.replace('https?:\\/\\/[^\\s]*','').str.replace(\"’\",\"'\").str.replace(\"''\",' ').str.replace(\"'\",\" ' \").str.replace('“','\"').str.replace('”','\"').str.replace('\"',' : ').str.replace('.',' . ').str.replace(',',' , ').str.replace('[',' [ ').str.replace(']',' ] ').str.replace('(',' ( ').str.replace(')',' ) ').str.replace('!',' ! ').str.replace('?',' ? ').str.replace(';',' ').str.replace(':',' ').str.replace('-',' - ').str.replace('=', ' ').str.replace('=', ' ').str.replace('*', ' ').str.replace('|', ' ').str.replace('«', ' ').str.replace('\\d', ' ').str.replace('\\n', ' ').str.replace('\\s\\s+',' ').str.strip().str.replace(\"[^\\s\\d\\w)(;:\\-+_?!\\]\\[,*,'\\\"]+\",\"\")\n",
    "\n",
    "    youtube_gb_comments.drop(['video_id','likes','replies'],1, inplace=True)\n",
    "    youtube_us_comments.drop(['video_id','likes','replies'],1, inplace=True)\n",
    "    return pd.concat([youtube_gb_comments,youtube_us_comments])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_toxicity_df():\n",
    "    toxicity_annotated_comments = pd.read_csv('toxicity_annotated_comments.tsv', sep='\\t')\n",
    "    toxicity_annotations = pd.read_csv('toxicity_annotations.tsv', sep='\\t')\n",
    "    rev_toxicity = toxicity_annotations.drop(['worker_id'],1).groupby(['rev_id']).mean().reset_index()\n",
    "    toxicity_df = toxicity_annotated_comments.merge(rev_toxicity, left_on='rev_id', right_on='rev_id', how='inner')[['comment','toxicity']]\n",
    "    # add toxicity to train\n",
    "    # reshape toxicity into train\n",
    "    toxicity_df = toxicity_df.rename(index=str, columns={'comment': 'comment_text'})\n",
    "    toxicity_df['toxic'] = toxicity_df['toxicity'] > 0.5\n",
    "    toxicity_df['toxic'] = toxicity_df['toxic'].apply(lambda x: 1 if x else 0)\n",
    "    toxicity_df['id'] = 0\n",
    "    toxicity_df['severe_toxic'] = 0\n",
    "    toxicity_df['obscene'] = 0\n",
    "    toxicity_df['threat'] = 0\n",
    "    toxicity_df['insult'] = 0\n",
    "    toxicity_df['identity_hate'] = 0\n",
    "    toxicity_df[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "    toxicity_df[COMMENT] = toxicity_df[COMMENT].str.lower().str.replace('https?:\\/\\/[^\\s]*','').str.replace(\"’\",\"'\").str.replace(\"''\",' ').str.replace(\"'\",\" ' \").str.replace('“','\"').str.replace('”','\"').str.replace('\"',' : ').str.replace('.',' . ').str.replace(',',' , ').str.replace('[',' [ ').str.replace(']',' ] ').str.replace('(',' ( ').str.replace(')',' ) ').str.replace('!',' ! ').str.replace('?',' ? ').str.replace(';',' ').str.replace(':',' ').str.replace('-',' - ').str.replace('=', ' ').str.replace('=', ' ').str.replace('*', ' ').str.replace('|', ' ').str.replace('«', ' ').str.replace('\\d', ' ').str.replace('\\n', ' ').str.replace('\\s\\s+',' ').str.strip().str.replace(\"[^\\s\\d\\w)(;:\\-+_?!\\]\\[,*,'\\\"]+\",\"\")\n",
    "    toxicity_df.drop(['toxicity'],1,inplace=True)\n",
    "    toxicity_df['clean'] = 1-toxicity_df[label_cols].max(axis=1)\n",
    "    return toxicity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_comment = train.copy()\n",
    "train_comment.drop(label_cols, 1, inplace=True)\n",
    "train_comment.drop(['clean'],1, inplace=True)\n",
    "train_comment.to_csv(\"train_comments.csv\", columns=['comment_text'], index = False)\n",
    "all_comment = pd.concat([train_comment,test])\n",
    "all_comment.to_csv('all_comments.csv', columns=['comment_text'], index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_wiki_youtube_comment = pd.concat([train_comment,test,youtube_df])\n",
    "all_wiki_youtube_comment.to_csv('all_wiki_youtube_comments.csv', columns=['comment_text'], index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText()\n",
    "#model.skipgram(input='all_comments.csv', output='comments_skipgram', minn='1', maxn='5', wordNgrams='3')\n",
    "model.skipgram(input='all_wiki_youtube_comments.csv', output='awy_comments_skipgram', minn='1', maxn='5', wordNgrams='3', epoch='1')\n",
    "model = FastText()\n",
    "del model\n",
    "#model.skipgram(input='train_comments.csv', output='train_comments_skipgram', minn='1', maxn='5', wordNgrams='2')\n",
    "#del model\n",
    "#model1 = FastText()\n",
    "#model1.cbow(input='train_comments.csv', output='train_comments_cbow', minn='1', maxn='5', wordNgrams='2')\n",
    "#del model1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/pandas/core/indexing.py:537: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "train_df = None\n",
    "val_df = None\n",
    "toxicity_df = get_toxicity_df() \n",
    "for label in label_cols:\n",
    "    train['label'] = \"\"    \n",
    "    train_df, val_df = train_test_split(train, test_size=0.2, random_state=1)\n",
    "    # merge toxicity before training\n",
    "    train_df = pd.concat([train_df, toxicity_df])\n",
    "    train_df.loc[(train_df[label]==1), 'label'] = \"__label__\" + label + \" \"\n",
    "    train_df.loc[(train_df[label]==0), 'label'] = \"__label__clean\" + \" \"\n",
    "    val_df.loc[(val_df[label]==1), 'label'] = \"__label__\" + label + \" \"\n",
    "    val_df.loc[(val_df[label]==0), 'label'] = \"__label__clean\" + \" \"\n",
    "    train_df.to_csv(\"fasttext_train_\"+label+\".csv\", columns=['label', 'comment_text'], index=False)    \n",
    "    train.to_csv(\"fasttext_train_all_\"+label+\".csv\", columns=['label', 'comment_text'], index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_name(target, param):\n",
    "    name = target\n",
    "    for key in param.keys():\n",
    "        name += \"_\"+key+str(param[key])        \n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(param, target, all_sample = False):\n",
    "    model = FastText()    \n",
    "    if all_sample == False:\n",
    "        #model.supervised(input='fasttext_train_'+target+'.csv', output='parameter_search', epoch=param[\"epoch\"], minn=param[\"minn\"], maxn=param[\"maxn\"], wordNgrams=param[\"wordNgrams\"], dim=param[\"dim\"], pretrainedVectors='awy_comments_skipgram.vec')\n",
    "        model.supervised(input='fasttext_train_'+target+'.csv', output='parameter_search', epoch=param[\"epoch\"], minn=param[\"minn\"], maxn=param[\"maxn\"], wordNgrams=param[\"wordNgrams\"], dim=param[\"dim\"])\n",
    "    else:\n",
    "        model.supervised(input='fasttext_train_all_'+target+'.csv', output='parameter_search', epoch=param[\"epoch\"], minn=param[\"minn\"], maxn=param[\"maxn\"], wordNgrams=param[\"wordNgrams\"], dim=param[\"dim\"], pretrainedVectors='comments_skipgram.vec')        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_log_loss(model, target):\n",
    "    predict_probas = model.predict_proba(val_df[COMMENT],k=2) \n",
    "    pred = []\n",
    "    for predict in predict_probas:\n",
    "        pred_prob = 0\n",
    "        for label, prob in predict:\n",
    "            if label == target:         \n",
    "                pred_prob = prob\n",
    "        pred.append(pred_prob) \n",
    "    return log_loss(val_df[target],pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter search toxic\n",
      "parameter search severe_toxic\n",
      "parameter search obscene\n",
      "parameter search threat\n",
      "parameter search insult\n",
      "parameter search identity_hate\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "param_grid = {}\n",
    "param_grid[\"minn\"] = [1,2]\n",
    "param_grid[\"maxn\"] = [2,3,4,5]\n",
    "param_grid[\"epoch\"] = [5,6,7,8,10,15]\n",
    "param_grid[\"lr\"] = [0.1,0.5,1]\n",
    "param_grid[\"wordNgrams\"] =[1,2,3,4,5]\n",
    "param_grid[\"dim\"] =[100,150,200]\n",
    "targets = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "log_loss_df_columns = ['target','log_loss','model_name']\n",
    "for key in param_grid.keys():\n",
    "    log_loss_df_columns.append(key)\n",
    "\n",
    "log_loss_df = None\n",
    "try:\n",
    "    log_loss_df = pd.read_csv('parameter_random_search.csv')    \n",
    "except:\n",
    "    print(\"parameter_random_search.csv not found\")\n",
    "    log_loss_df = pd.DataFrame(columns=log_loss_df_columns)\n",
    "\n",
    "tested_params = log_loss_df['model_name'].tolist()\n",
    "\n",
    "for target in targets:\n",
    "    print(\"parameter search\",target)    \n",
    "    max_test = len(tested_params) + 10\n",
    "    while len(tested_params) < max_test:        \n",
    "        param = random.choice(list(ParameterGrid(param_grid)))                     \n",
    "        test_model_name = model_name(target, param)\n",
    "        if test_model_name in tested_params:\n",
    "            continue\n",
    "        else:\n",
    "            tested_params.append(test_model_name)\n",
    "            \n",
    "        model = train_model(param, target)\n",
    "        score = calculate_log_loss(model, target)\n",
    "        \n",
    "        log_loss_df_row = {}\n",
    "        log_loss_df_row['target'] = target\n",
    "        log_loss_df_row['log_loss'] = score\n",
    "        log_loss_df_row['model_name'] = test_model_name\n",
    "        for key in param.keys():\n",
    "            log_loss_df_row[key] = param[key]\n",
    "        log_loss_df = log_loss_df.append(log_loss_df_row, ignore_index=True)        \n",
    "        log_loss_df.to_csv(\"parameter_random_search.csv\", index=False)    \n",
    "        print(score, param)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter search toxic\n",
      "0.144929079678 {'wordNgrams': 2, 'dim': 100, 'minn': 1, 'epoch': 6, 'lr': 0.5, 'maxn': 5}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {}\n",
    "param_grid[\"minn\"] = [1]\n",
    "param_grid[\"maxn\"] = [5]\n",
    "param_grid[\"epoch\"] = [6]\n",
    "param_grid[\"lr\"] = [0.5]\n",
    "param_grid[\"wordNgrams\"] =[2]\n",
    "param_grid[\"dim\"] =[100]\n",
    "#targets = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "targets = ['toxic']\n",
    "\n",
    "log_loss_df_columns = ['target','log_loss']\n",
    "for key in param_grid.keys():\n",
    "    log_loss_df_columns.append(key)\n",
    "\n",
    "log_loss_df = pd.DataFrame(columns=log_loss_df_columns)\n",
    "\n",
    "for target in targets:\n",
    "    print(\"parameter search\",target)    \n",
    "    for param in list(ParameterGrid(param_grid)):                     \n",
    "        model = train_model(param, target)\n",
    "        score = calculate_log_loss(model, target)\n",
    "        \n",
    "        log_loss_df_row = {}\n",
    "        log_loss_df_row['target'] = target\n",
    "        log_loss_df_row['log_loss'] = score\n",
    "        for key in param.keys():\n",
    "            log_loss_df_row[key] = param[key]\n",
    "        log_loss_df = log_loss_df.append(log_loss_df_row, ignore_index=True)        \n",
    "        log_loss_df.to_csv(\"parameter_grid_search.csv\", index=False)    \n",
    "        print(score, param)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1012490261837956"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss_df.groupby('target')['log_loss'].min().sum()/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "identity_hate    0.040433\n",
       "insult           0.142292\n",
       "obscene          0.134799\n",
       "severe_toxic     0.041574\n",
       "threat           0.018249\n",
       "toxic            0.230147\n",
       "Name: log_loss, dtype: float64"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss_df.groupby('target')['log_loss'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>log_loss</th>\n",
       "      <th>lr</th>\n",
       "      <th>epoch</th>\n",
       "      <th>maxn</th>\n",
       "      <th>minn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.040433</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.040692</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.041091</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.041208</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.041644</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.042095</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.042737</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.042974</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.042981</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.043320</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.043448</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.043522</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.043793</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.043804</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.043834</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.043938</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.044063</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.044079</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.044621</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.044756</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.044782</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.045082</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.045225</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.045742</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.046618</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.047210</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.047718</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.047982</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.048113</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.048283</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.048338</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.048650</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.048809</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.049102</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.049166</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.049249</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.049499</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.049576</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.049759</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.049908</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.051049</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.051494</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.051546</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.052387</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.054833</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            target  log_loss   lr epoch maxn minn\n",
       "256  identity_hate  0.040433  0.1     5    3    2\n",
       "261  identity_hate  0.040692  0.1     5    5    1\n",
       "241  identity_hate  0.041091  0.1     3    3    2\n",
       "258  identity_hate  0.041208  0.1     5    4    1\n",
       "229  identity_hate  0.041644  0.1     2    4    2\n",
       "264  identity_hate  0.042095  0.1     5    6    1\n",
       "246  identity_hate  0.042737  0.1     3    5    1\n",
       "267  identity_hate  0.042974  0.1     5    7    1\n",
       "243  identity_hate  0.042981  0.1     3    4    1\n",
       "249  identity_hate  0.043320  0.1     3    6    1\n",
       "226  identity_hate  0.043448  0.1     2    3    2\n",
       "259  identity_hate  0.043522  0.1     5    4    2\n",
       "255  identity_hate  0.043793  0.1     5    3    1\n",
       "234  identity_hate  0.043804  0.1     2    6    1\n",
       "252  identity_hate  0.043834  0.1     3    7    1\n",
       "231  identity_hate  0.043938  0.1     2    5    1\n",
       "228  identity_hate  0.044063  0.1     2    4    1\n",
       "262  identity_hate  0.044079  0.1     5    5    2\n",
       "265  identity_hate  0.044621  0.1     5    6    2\n",
       "244  identity_hate  0.044756  0.1     3    4    2\n",
       "237  identity_hate  0.044782  0.1     2    7    1\n",
       "240  identity_hate  0.045082  0.1     3    3    1\n",
       "268  identity_hate  0.045225  0.1     5    7    2\n",
       "247  identity_hate  0.045742  0.1     3    5    2\n",
       "225  identity_hate  0.046618  0.1     2    3    1\n",
       "266  identity_hate  0.047210  0.1     5    6    3\n",
       "245  identity_hate  0.047718  0.1     3    4    3\n",
       "250  identity_hate  0.047982  0.1     3    6    2\n",
       "263  identity_hate  0.048113  0.1     5    5    3\n",
       "232  identity_hate  0.048283  0.1     2    5    2\n",
       "269  identity_hate  0.048338  0.1     5    7    3\n",
       "260  identity_hate  0.048650  0.1     5    4    3\n",
       "253  identity_hate  0.048809  0.1     3    7    2\n",
       "248  identity_hate  0.049102  0.1     3    5    3\n",
       "235  identity_hate  0.049166  0.1     2    6    2\n",
       "227  identity_hate  0.049249  0.1     2    3    3\n",
       "238  identity_hate  0.049499  0.1     2    7    2\n",
       "242  identity_hate  0.049576  0.1     3    3    3\n",
       "230  identity_hate  0.049759  0.1     2    4    3\n",
       "251  identity_hate  0.049908  0.1     3    6    3\n",
       "254  identity_hate  0.051049  0.1     3    7    3\n",
       "233  identity_hate  0.051494  0.1     2    5    3\n",
       "257  identity_hate  0.051546  0.1     5    3    3\n",
       "236  identity_hate  0.052387  0.1     2    6    3\n",
       "239  identity_hate  0.054833  0.1     2    7    3"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "log_loss_df[log_loss_df['target']=='identity_hate'].sort_values('log_loss',ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['clean', 'toxic']\n",
      "['clean', 'severe_toxic']\n",
      "['clean', 'obscene']\n",
      "['clean', 'threat']\n",
      "['clean', 'insult']\n",
      "['clean', 'identity_hate']\n"
     ]
    }
   ],
   "source": [
    "comment_ids = []\n",
    "comments = []\n",
    "for index, row in test.iterrows():\n",
    "    comments.append(row['comment_text'])\n",
    "    comment_ids.append(row['id'])\n",
    "\n",
    "predict_probas = {}\n",
    "param = {}\n",
    "param['toxic'] = {'maxn': 5, 'wordNgrams': 3, 'lr': 0.5, 'minn': 1, 'epoch': 8, 'dim': 100}\n",
    "param['severe_toxic'] = {'maxn': 5, 'wordNgrams': 4, 'lr': 0.5, 'minn': 2, 'epoch': 5, 'dim': 100}\n",
    "param['obscene'] = {'maxn': 5, 'wordNgrams': 1, 'lr': 0.1, 'minn': 1, 'epoch': 8, 'dim': 100}\n",
    "param['threat'] = {'maxn': 4, 'wordNgrams': 5, 'lr': 1, 'minn': 1, 'epoch': 10, 'dim': 100}\n",
    "param['insult'] = {'maxn': 5, 'wordNgrams': 3, 'lr': 0.5, 'minn': 1, 'epoch': 5, 'dim': 100}\n",
    "param['identity_hate'] = {'maxn': 5, 'wordNgrams': 3, 'lr': 1, 'minn': 2, 'epoch': 6, 'dim': 100}\n",
    "\n",
    "for target in label_cols:\n",
    "    model = train_model(param[target], target, all_sample = True)\n",
    "    print(model.labels)\n",
    "    predict_probas[target] = model.predict_proba(comments,k=2)   \n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('submit.csv', \"w\") as csv_file:\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    writer.writerow(['id','toxic','severe_toxic','obscene','threat','insult','identity_hate'])\n",
    "    csv_rows = []\n",
    "    for index, comment_id in enumerate(comment_ids):\n",
    "        #print('processing',index)\n",
    "        prob_dict = {}        \n",
    "        #print(comments[index])\n",
    "        for label in label_cols:       \n",
    "            #print(predict_probas[label][index])            \n",
    "            for label_predict, prob in predict_probas[label][index]:\n",
    "                if label_predict != 'clean':    \n",
    "                    prob_dict[label] = prob        \n",
    "        csv_row=[]\n",
    "        csv_row.append(comment_id)\n",
    "        \n",
    "        # severe_toxic -> toxic trick                \n",
    "        #if 'severe_toxic' in prob_dict and 'toxic' in prob_dict and prob_dict['severe_toxic'] > prob_dict['toxic']:            \n",
    "        #    prob_dict['toxic'] = prob_dict['severe_toxic']            \n",
    "        \n",
    "        for label in label_cols:\n",
    "            if label in prob_dict:\n",
    "                csv_row.append(prob_dict[label])\n",
    "            else:\n",
    "                csv_row.append(0)\n",
    "                #print('no prediction:',index,'comment:',comments[index])\n",
    "                #csv_row.append(prob_dict[label])\n",
    "        csv_rows.append(csv_row)    \n",
    "    writer.writerows(csv_rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
