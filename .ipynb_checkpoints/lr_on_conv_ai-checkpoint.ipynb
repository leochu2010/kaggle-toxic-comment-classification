{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a basic LogisticRegression model trained using the data from https://www.kaggle.com/eoveson/convai-datasets-baseline-models\n",
    "\n",
    "The baseline model in that kernal is tuned a little to get the data for this kernal This kernal scored 0.044 in the LB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "eb9acbb1-40db-4a60-9c00-7e1134408cb1",
    "_uuid": "7e97dad72af19207237cb816bc898ca5818f4389"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_comments.txt\n",
      "glove.6B.50d.txt\n",
      "glove.6B.50d.txt.zip\n",
      "sample_submission.csv\n",
      "sample_submission.csv.zip\n",
      "test_comments.csv\n",
      "test_comments_no_title.csv\n",
      "test.csv\n",
      "test.csv.zip\n",
      "train_comments.csv\n",
      "train_comments_no_title.csv\n",
      "train.csv\n",
      "train.csv.zip\n",
      "train_test_comments.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy import sparse\n",
    "# set stopwords\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"input\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "bb967e03-d30b-46ec-b9d2-c0f5d4c0ee68",
    "_uuid": "97b399586c43626b73bc77b50e58b952d86ea8da"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('input/dataset/train_with_convai.csv')\n",
    "test = pd.read_csv('input/dataset/test_with_convai.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "1eebb207-607e-4985-908e-9848888808b1",
    "_uuid": "3e90295dde0dd25158ea9e3464165aa8ea62fd1c"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['toxic_level' 'attack' 'aggression'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-837d28e065a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfeats_to_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'comment_text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'toxic_level'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'attack'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'aggression'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# combining test and train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0malldata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeats_to_concat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeats_to_concat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0malldata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomment_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unknown'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2132\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2133\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2134\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2135\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2175\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2176\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2178\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1267\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m                     raise KeyError('{mask} not in index'\n\u001b[0;32m-> 1269\u001b[0;31m                                    .format(mask=objarr[mask]))\n\u001b[0m\u001b[1;32m   1270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['toxic_level' 'attack' 'aggression'] not in index\""
     ]
    }
   ],
   "source": [
    "feats_to_concat = ['comment_text', 'toxic_level', 'attack', 'aggression']\n",
    "# combining test and train\n",
    "alldata = pd.concat([train[feats_to_concat], test[feats_to_concat]], axis=0)\n",
    "alldata.comment_text.fillna('unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "88a8e609-b287-4a7e-b72d-5dcac6f4a55f",
    "_uuid": "741273ee4b5122a37d978708ba29e16879e5b33f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect_words = TfidfVectorizer(max_features=50000, analyzer='word', ngram_range=(1, 1))\n",
    "vect_chars = TfidfVectorizer(max_features=20000, analyzer='char', ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "6db22032-8e99-4848-8978-be7c68a1e936",
    "_uuid": "cf10b99072cef22bf87ee92c9aa51f035a26e893",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words = vect_words.fit_transform(alldata.comment_text)\n",
    "all_chars = vect_chars.fit_transform(alldata.comment_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "8f42e0d7-5938-4bb0-beb7-7ddf9f85685d",
    "_uuid": "d074b6b6c5271f462c129c534980c5a0d287599f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_new = train\n",
    "test_new = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "c068c9bb-bf28-4342-aa71-e575c6d93788",
    "_uuid": "09975f14757c51e19876dab638a39671dfd555e4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_words = all_words[:len(train_new)]\n",
    "test_words = all_words[len(train_new):]\n",
    "\n",
    "train_chars = all_chars[:len(train_new)]\n",
    "test_chars = all_chars[len(train_new):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "5d55e152-e1cb-4cf0-aa41-e3eec5850b3a",
    "_uuid": "0338f2d0b8f09c751f97afebf1cf8e77d8a10fe3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feats = ['toxic_level', 'attack']\n",
    "# make sparse matrix with needed data for train and test\n",
    "train_feats = sparse.hstack([train_words, train_chars, alldata[feats][:len(train_new)]])\n",
    "test_feats = sparse.hstack([test_words, test_chars, alldata[feats][len(train_new):]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "350aad79-ee6f-44bc-9d85-4e9652956bd3",
    "_uuid": "da2082c68a367369fac28ddc09eec2e5b6c718bb",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Fit toxic\n",
      "Fitting model\n",
      "Predicting on test\n",
      "===Fit severe_toxic\n",
      "Fitting model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on test\n",
      "===Fit obscene\n",
      "Fitting model\n",
      "Predicting on test\n",
      "===Fit threat\n",
      "Fitting model\n",
      "Predicting on test\n",
      "===Fit insult\n",
      "Fitting model\n",
      "Predicting on test\n",
      "===Fit identity_hate\n",
      "Fitting model\n",
      "Predicting on test\n"
     ]
    }
   ],
   "source": [
    "col = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "only_col = ['toxic']\n",
    "\n",
    "preds = np.zeros((test_new.shape[0], len(col)))\n",
    "\n",
    "for i, j in enumerate(col):\n",
    "    print('===Fit '+j)\n",
    "    \n",
    "    model = LogisticRegression(C=4.0, solver='sag')\n",
    "    print('Fitting model')\n",
    "    model.fit(train_feats, train_new[j])\n",
    "      \n",
    "    print('Predicting on test')\n",
    "    preds[:,i] = model.predict_proba(test_feats)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "9d84b909-d93b-4778-b432-701f65a73d3c",
    "_uuid": "3605ca797e6d5e4d05ac2c63d70766c23d2a8cf1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subm = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv')\n",
    "\n",
    "submid = pd.DataFrame({'id': subm[\"id\"]})\n",
    "submission = pd.concat([submid, pd.DataFrame(preds, columns = col)], axis=1)\n",
    "submission.to_csv('feat_lr_2cols.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "6d350714-1262-4f91-af11-a7f95750ec84",
    "_uuid": "be385cfe2683246d05dc872d7b09cb4608b73337",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
